{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dd4b73-b94a-4321-af62-4559f05e89ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d03337-cf41-4e51-9e36-70ee9709f8a6",
   "metadata": {},
   "source": [
    "### Downloading and Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fad8c51-5ebf-4567-8394-7e3bda5f8424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_html_sample(url):\n",
    "    original_photo=requests.get(url)\n",
    "    with Image.open(BytesIO(original_photo.content)) as img:\n",
    "        data_img=np.array(img)\n",
    "    return data_img\n",
    "\n",
    "def img_cut(data):\n",
    "    \n",
    "    top_left_vertical = 60\n",
    "    top_left_horizontal = 100\n",
    "    height = 480\n",
    "    width = 575\n",
    "\n",
    "    processed = data[top_left_vertical:top_left_vertical+height, top_left_horizontal:top_left_horizontal+width]\n",
    "    \n",
    "    return processed\n",
    "\n",
    "def gray_url(url):\n",
    "    img=img_cut(load_html_sample(url))\n",
    "    img = 0.2989 * img[:,:,0] + 0.5870 * img[:,:,1] + 0.1140 * img[:,:,2]\n",
    "    return img\n",
    "\n",
    "def gray_scale(img):\n",
    "    r, g, b = img[:,:,0], img[:,:,1], img[:,:,2]\n",
    "    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "    return gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c375c38-1e2e-4b44-956f-3f9c27a97935",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def concat_csv_in_folder(folder_path):\n",
    "    csv_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "    df_concatenated = pd.concat((pd.read_csv(f) for f in csv_files), ignore_index=True)\n",
    "    return df_concatenated\n",
    "\n",
    "## directory\n",
    "data = concat_csv_in_folder(\"/Users/jianggh/Desktop/Gravity Spy Project/Gravity Spy Dataset/Data\")\n",
    "\n",
    "data = data[['ml_label','ml_confidence','url1','url2','url3','url4']]\n",
    "\n",
    "\n",
    "#Change ml_label column into one hot expression.\n",
    "unique_values = data['ml_label'].unique()\n",
    "\n",
    "data.dropna()\n",
    "print(data.info())\n",
    "\n",
    "data['ml_confidence'] = data['ml_confidence'].astype(float)\n",
    "print(data['ml_label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ee537c-b4ca-495c-b0db-8c3842327e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scattered_light_data = data[data['ml_label'] == 'Wandering_Line']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(scattered_light_data['ml_confidence'], bins=int((scattered_light_data['ml_confidence'].max() - scattered_light_data['ml_confidence'].min()) / 0.01), edgecolor='black')\n",
    "plt.title('Distribution of ml_confidence for Scattered_Light')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dbbbed-4b54-4efc-8a5c-7ab86707d480",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pd.DataFrame()\n",
    "\n",
    "grouped = data.groupby('ml_label')\n",
    "\n",
    "for name, group in grouped:\n",
    "\n",
    "    if len(group)<=250:\n",
    "        sampled_group = group\n",
    "        samples = pd.concat([samples, sampled_group])\n",
    "    else:\n",
    "        filtered_group = group[group['ml_confidence'] > 0.90]\n",
    "        samples = pd.concat([samples, filtered_group])\n",
    "\n",
    "print(f\"Total sampled rows: {len(samples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bdee9d-d36a-4a65-b664-bca83c438075",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = samples.reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a0a33f-a86c-4471-bf58-26dc0a2d69a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.to_csv(\"/Volumes/姜家大备份/GS Testing Dataset/0.samples.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f52bb96-a52c-4af2-8661-3775e246fea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/Volumes/姜家大备份/GS Testing Dataset/\"\n",
    "\n",
    "folder_names = [\n",
    "    \"Whistle\", \"Tomte\", \"Low_Frequency_Burst\", \"Fast_Scattering\", \"Scattered_Light\",\n",
    "    \"Low_Frequency_Lines\", \"Scratchy\", \"1080Lines\", \"Blip_Low_Frequency\", \"Power_Line\",\n",
    "    \"Repeating_Blips\", \"Blip\", \"Extremely_Loud\", \"Koi_Fish\", \"Light_Modulation\",\n",
    "    \"Violin_Mode\", \"Helix\", \"No_Glitch\", \"1400Ripples\", \"Chirp\", \"Wandering_Line\",\n",
    "    \"Air_Compressor\", \"Paired_Doves\", \"None_of_the_Above\"\n",
    "]\n",
    "\n",
    "for folder_name in folder_names:\n",
    "    folder_path = os.path.join(base_path, folder_name)\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feab11a-2f0d-43fa-b396-7488b2753b7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gray_img(arr):\n",
    "    img=img_cut(arr)\n",
    "    img = 0.2989 * img[:,:,0] + 0.5870 * img[:,:,1] + 0.1140 * img[:,:,2]\n",
    "    img = np.round(img).astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "error_list=[]\n",
    "for index, row in samples[:].iterrows():\n",
    "    print(index)\n",
    "    ml_label = row['ml_label']\n",
    "    base_dir = f\"/Volumes/姜家大备份/GS Testing Data/{ml_label}\"\n",
    "    index_dir = f\"{base_dir}/{index}\"\n",
    "    \n",
    "    if not os.path.exists(base_dir):\n",
    "        os.makedirs(base_dir)\n",
    "    \n",
    "    if not os.path.exists(index_dir):\n",
    "        os.makedirs(index_dir)\n",
    "    \n",
    "    for i, url in enumerate([row['url1'], row['url2'], row['url3'], row['url4']], start=1):\n",
    "        try:\n",
    "            img = requests.get(url)\n",
    "            img.raise_for_status()\n",
    "            with Image.open(BytesIO(img.content)) as img:\n",
    "                img=np.array(img)\n",
    "            img = gray_img(img)\n",
    "            np.save(f\"{index_dir}/url{i}.npy\", img)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request Error at Index: {index} URL: url{i}\")\n",
    "            error_list.append((index,i))\n",
    "        except requests.exceptions.HTTPError as http_err:\n",
    "            print(f\"HTTP error occurred at Index: {index} URL: url{i}\")\n",
    "            error_list.append((index,i))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image at Index: {index} URL: url{i}\")\n",
    "            error_list.append((index,i))            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04152f82-ee8a-46c9-a966-20ecb971a285",
   "metadata": {},
   "source": [
    "### Retrying failed images, else removing the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78773edc-1ab9-432e-8ecc-d5b2f1021576",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "second_error_list = []\n",
    "for (i, j) in error_list:\n",
    "    print(i,j)\n",
    "    ml_label = samples.loc[i,'ml_label']\n",
    "    base_dir = f\"/Volumes/姜家大备份/GS Testing Data/{ml_label}\"\n",
    "    index_dir = f\"{base_dir}/{i}\"\n",
    "    \n",
    "    file_path = f\"{index_dir}/url{j}.npy\"\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"File already exists\")\n",
    "        continue\n",
    "    \n",
    "    if not os.path.exists(base_dir):\n",
    "        os.makedirs(base_dir)\n",
    "    \n",
    "    if not os.path.exists(index_dir):\n",
    "        os.makedirs(index_dir)\n",
    "    \n",
    "    url = samples.loc[i,f'url{j}']\n",
    "    try:\n",
    "        img = requests.get(url)\n",
    "        img.raise_for_status()\n",
    "        with Image.open(BytesIO(img.content)) as img:\n",
    "            img=np.array(img)\n",
    "        img = gray_img(img)\n",
    "        np.save(f\"{index_dir}/url{j}.npy\", img)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request Error at Index: {i} URL: url{j}\")\n",
    "        second_error_list.append((i,j))\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred at Index: {i} URL: url{j}\")\n",
    "        second_error_list.append((i,j))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image at Index: {i} URL: url{j}\")\n",
    "        second_error_list.append((i,j))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41757511-44c1-4dcc-b34b-74893a47a397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "deletion_error_list = []\n",
    "for i, j in second_error_list:\n",
    "    ml_label = samples.loc[i, 'ml_label']\n",
    "    base_dir = f\"/Volumes/姜家大备份/GS Testing Data/{ml_label}\"\n",
    "    index_dir = f\"{base_dir}/{i}\"\n",
    "    \n",
    "    if os.path.exists(index_dir):\n",
    "        try:\n",
    "            shutil.rmtree(index_dir)\n",
    "            print(f\"Deleted directory: {index_dir}\")\n",
    "        except Exception as e:\n",
    "            deletion_error_list.append((i,j))\n",
    "            print(f\"Error deleting directory {index_dir}: {e}\")\n",
    "    else:\n",
    "        print(f\"Directory does not exist: {index_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b504873-1868-4974-85e9-c876b19f53b6",
   "metadata": {},
   "source": [
    "### Checking if all data is proper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1485e632-9a53-4670-a4dd-763ca68699b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_csv_files(Class_Folder):\n",
    "    for Class_Root, Index_Level_Folders, files in os.walk(Class_Folder):\n",
    "        for index in Index_Level_Folders:\n",
    "            print(index)\n",
    "            Index_Folder = os.path.join(Class_Root,index)\n",
    "            if int(index) >= 6000:\n",
    "                continue\n",
    "            else:\n",
    "                for Index_Root, folders, NPY_Files in os.walk(Index_Folder): \n",
    "                    for file in NPY_Files:\n",
    "                        file_path = os.path.join(Index_Root, file)\n",
    "                        arr = np.load(file_path)\n",
    "                        arr = np.round(arr).astype(np.uint8)\n",
    "                        np.save(file_path, arr)\n",
    "\n",
    "def traverse_and_process(GS_Selected_Data):\n",
    "    for GS_Selected_Data_Root, Class_Level_Folders, files in os.walk(GS_Selected_Data):\n",
    "        for Class_Folder in Class_Level_Folders:\n",
    "            print(Class_Folder)\n",
    "            \n",
    "            dir_path = os.path.join(GS_Selected_Data_Root, Class_Folder)\n",
    "\n",
    "            process_csv_files(dir_path)\n",
    "\n",
    "base_dir = '/Volumes/姜家大备份/GS Selected Data'\n",
    "traverse_and_process(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff07d5f-e5b7-4012-ae25-db13d8430dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#检查文件是否是有且仅有四个url，并且命名正确\n",
    "base_dir = '/Volumes/姜家大备份/GS Selected Data'\n",
    "problem_indices = []\n",
    "\n",
    "for dataroot, class_folders, _ in os.walk(base_dir):  # GS Selected Data 里面的所有文件夹\n",
    "    for class_ in class_folders:  # 遍历所有类别文件夹\n",
    "        class_dir = os.path.join(dataroot, class_)\n",
    "        for index in os.listdir(class_dir):  # 特定类别里面的所有文件夹（数据索引）\n",
    "            item_dir = os.path.join(class_dir, index)\n",
    "            if os.path.isdir(item_dir):  # 确保是目录\n",
    "                urls = os.listdir(item_dir)\n",
    "                expected_files = {\"url1.npy\", \"url2.npy\", \"url3.npy\", \"url4.npy\"}\n",
    "                if len(urls) != 4 or not expected_files.issubset(set(urls)):\n",
    "                    problem_indices.append(index)\n",
    "                    print(index)\n",
    "\n",
    "problem_indices = [int(item) for item in problem_indices]\n",
    "print(problem_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834870df-82a9-4273-9d2a-3ce33c7d4d8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#先删除所有problem_indices，之后在上面重新下载一遍problem_indices\n",
    "import shutil\n",
    "\n",
    "deletion_error_list = []\n",
    "for i in problem_indices:\n",
    "    ml_label = selected_data.loc[i, 'ml_label']\n",
    "    base_dir = f\"/Volumes/姜家大备份/GS Selected Data/{ml_label}\"\n",
    "    index_dir = f\"{base_dir}/{i}\"\n",
    "    \n",
    "    if os.path.exists(index_dir):\n",
    "        try:\n",
    "            shutil.rmtree(index_dir)\n",
    "            print(f\"Deleted directory: {index_dir}\")\n",
    "        except Exception as e:\n",
    "            deletion_error_list.append((i,j))\n",
    "            print(f\"Error deleting directory {index_dir}: {e}\")\n",
    "    else:\n",
    "        print(f\"Directory does not exist: {index_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
