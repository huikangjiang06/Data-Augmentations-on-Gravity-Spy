{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9702762-e73c-487f-995d-74c2c3951f95",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e32c5d-0876-4fd8-b472-2cc2c3720fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "from ipynb.fs.defs.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405324bf-971d-468b-8939-9982a0a90da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"/Users/jianggh/Desktop/Gravity Spy Dataset/Data/H1_O1.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aea4b3-f318-460a-900f-9c979a653100",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "print(len(df.columns))\n",
    "print(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692c41e4-69f1-432f-9e41-f67d989bce02",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_url=df.loc[1000,'url1']\n",
    "print(example_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454431e6-642d-422a-b3be-539236da674b",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = gray_scale(img_cut(load_html_sample(example_url)))\n",
    "\n",
    "example = Image.fromarray(example)\n",
    "\n",
    "example.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f2dbd7-0ee0-4b9a-aec3-4f8bb73dda3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3296dc03-768d-4885-9f07-b3c9d27722d8",
   "metadata": {},
   "source": [
    "# Model and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07176d68-6874-469c-b8a3-4a1008998518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from ipynb.fs.defs.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16deb1e6-4d1e-4e99-bf5a-449d218afc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassificationBase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))\n",
    "\n",
    "\n",
    "class Baseline1(ImageClassificationBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 16 x 128 x 128\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 32 x 64 x 64\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 64 x 32 x 32\n",
    "\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(64*32*32, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 196))\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)\n",
    "\n",
    "class Baseline2(nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Baseline2, self).__init__()\n",
    "\n",
    "        Nfilters = [8, 16, 16, 32, 64, 64, 128, 128]\n",
    "        filter_size = [(1, 32)] + [(1, 16)] * 3 + [(1, 8)] * 2 + [(1, 4)] * 2\n",
    "        filter_stride = [(1, 1)] * 8\n",
    "        dilation = [(1, 1)] * 8\n",
    "        pooling = [1, 0, 0, 0, 1, 0, 0, 1]\n",
    "        pool_size = [[1, 8]] + [(1, 1)] * 3 + [[1, 6]] + [(1, 1)] * 2 + [[1, 4]]\n",
    "        pool_stride = [[1, 8]] + [(1, 1)] * 3 + [[1, 6]] + [(1, 1)] * 2 + [[1, 4]]\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for i in range(8):\n",
    "            # 添加卷积层\n",
    "            self.layers.append(nn.Conv2d(\n",
    "                in_channels=1 if i == 0 else Nfilters[i-1],  # Number of channels in the input image\n",
    "                out_channels=Nfilters[i],  # Number of channels produced by the convolution\n",
    "                kernel_size=filter_size[i],  # Size of the convolving kernel\n",
    "                stride=filter_stride[i],  # Stride of the convolution\n",
    "                padding=0,  # Zero-padding added to both sides of the input\n",
    "                dilation=dilation[i],  # Spacing between kernel elements\n",
    "                groups=1,  # Number of blocked connections from input channels to output channels\n",
    "                bias=True,  # If True, adds a learnable bias to the output\n",
    "                padding_mode='zeros',  # Specifies the type of padding, 'zeros' pads with zero\n",
    "            ))\n",
    "            # 添加ELU激活函数，alpha参数为0.01\n",
    "            self.layers.append(nn.ELU(0.01))\n",
    "            # 添加批量归一化层，特征数量为Nfilters[i]\n",
    "            self.layers.append(nn.BatchNorm2d(num_features=Nfilters[i]))\n",
    "            # 如果pooling[i]为真，添加最大池化层\n",
    "            if pooling[i]:\n",
    "                # 最大池化层的参数：核大小为pool_size[i]，步长为pool_stride[i]，填充为0\n",
    "                self.layers.append(nn.MaxPool2d(\n",
    "                    kernel_size=pool_size[i],\n",
    "                    stride=pool_stride[i],\n",
    "                    padding=0,\n",
    "                ))\n",
    "\n",
    "        # 添加Flatten层，将输入展平\n",
    "        self.layers.append(nn.Flatten())\n",
    "        # 添加全连接层，输入维度为20224，输出维度为64\n",
    "        self.layers.append(nn.Linear(20224, 64))\n",
    "        # 添加ELU激活函数，alpha参数为0.01\n",
    "        self.layers.append(nn.ELU(0.01))\n",
    "        # 添加Dropout层，丢弃率为0.5\n",
    "        self.layers.append(nn.Dropout(0.5))\n",
    "        # 添加全连接层，输入维度为64，输出维度为2\n",
    "        self.layers.append(nn.Linear(64, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 前向传播函数\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class Baseline3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Baseline3, self).__init__()\n",
    "\n",
    "        #输入960*1150\n",
    "        self.features = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3),\n",
    "            nn.Dropout2d(0.5),\n",
    "            \n",
    "            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3),\n",
    "            nn.Dropout2d(0.5),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3),\n",
    "            nn.Dropout2d(0.5),\n",
    "            \n",
    "            nn.Conv2d(64, 64, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3),\n",
    "            nn.Dropout2d(0.5),\n",
    "\n",
    "            nn.Conv2d(64, 64, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(0.5),\n",
    "\n",
    "            nn.Conv2d(64, 64, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(0.5)\n",
    "            \n",
    "        )\n",
    "        #得到2*3\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 2 * 3, 256), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256,24),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f9fb33-ef06-4208-aa67-46fc7c246637",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "def evaluate_accuracy_gpu(net, data_iter, loss_func, device=None): \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    net.eval()\n",
    "    metric = Accumulator(3)\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            X = X.to(device).to(torch.float)\n",
    "            y = y.to(device).to(torch.long)\n",
    "            y_hat = net(X)\n",
    "            loss = loss_func(y_hat, y)\n",
    "            metric.add(num_accurate(y_hat, y), y.numel(), loss.sum())\n",
    "            \n",
    "            preds = torch.argmax(y_hat, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "            \n",
    "    return metric[0] / metric[1], metric[2] / metric[1], all_preds, all_labels\n",
    "\n",
    "def num_accurate(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return (preds == labels).sum().item()\n",
    "\n",
    "\n",
    "def save_model(epoch, model, optimizer, scheduler, checkpoint_dir, train_loss_history, filename):\n",
    "\n",
    "    p = Path(checkpoint_dir)\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    assert '.pt' in filename\n",
    "    for f in [f for f in os.listdir(p) if '.pt' in f]:\n",
    "        os.remove(p / f)\n",
    "\n",
    "    np.save(p / 'train_loss_history_cnn', train_loss_history)\n",
    "\n",
    "    output = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'optimizer_type': type(optimizer).__name__,\n",
    "        'epoch': epoch,\n",
    "    }\n",
    "\n",
    "    if scheduler is not None:\n",
    "        output['scheduler_state_dict'] = scheduler.state_dict()\n",
    "        output['scheduler_type'] = type(scheduler).__name__\n",
    "        \n",
    "    torch.save(output, p / filename)\n",
    "\n",
    "def load_model(checkpointdir):\n",
    "\n",
    "    net = MyNet()\n",
    "    \n",
    "    if checkpointdir is not None:\n",
    "        p = Path(checkpointdir)\n",
    "        if not p.is_dir():\n",
    "            print('Checkpoint Error')\n",
    "            return None\n",
    "    \n",
    "        files = [f for f in os.listdir(p) if f.endswith('.pt')]\n",
    "        if not files:\n",
    "            print('No model file found')\n",
    "            return None\n",
    "    \n",
    "        checkpoint_path = p / files[0]\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "    \n",
    "        net.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "        optimizer_type = checkpoint.get('optimizer_type')\n",
    "        if optimizer_type:\n",
    "            optimizer = getattr(optim, optimizer_type)(net.parameters())\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        else:\n",
    "            optimizer = None\n",
    "    \n",
    "        scheduler_type = checkpoint.get('scheduler_type')\n",
    "        if scheduler_type:\n",
    "            scheduler = getattr(optim.lr_scheduler, scheduler_type)(optimizer)\n",
    "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        else:\n",
    "            scheduler = None\n",
    "    \n",
    "        epoch = checkpoint['epoch']\n",
    "        train_loss_history = checkpoint['train_loss_history']\n",
    "    \n",
    "        print('Load Successful')\n",
    "        \n",
    "        return net, optimizer, scheduler, epoch, train_loss_history\n",
    "\n",
    "    else:\n",
    "        return net, 0, []\n",
    "\n",
    "def train(net, lr, epoch, total_epochs, dataset_train, data_loader, test_iter, train_loss_history, checkpoint_dir, device):\n",
    "    '''\n",
    "    net: 模型实例\n",
    "    lr: Learning Rate\n",
    "    epoch: 已经完成训练的epoch数量\n",
    "    total_epochs: 需要训练的epoch数量\n",
    "    dataset_train: 训练集\n",
    "    data_loader: DataLoader的实例, 用于加载训练集\n",
    "    test_iter: DataLoader的实力，用于加载训练集\n",
    "    train_loss_history: list, 损失随epoch变化的记录\n",
    "    checkpoint_dir: train loss history的输出地址\n",
    "    device: 用于训练的设备\n",
    "    '''\n",
    "\n",
    "    net.to(device)\n",
    "    \n",
    "    loss_func = nn.MSELoss()  \n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)  \n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                        optimizer,\n",
    "                        T_max=total_epochs,  \n",
    "                    )\n",
    "    \n",
    "    for epoch in range(epoch, epoch + total_epochs +1):\n",
    "\n",
    "        torch.cuda.empty_cache() \n",
    "        \n",
    "        metric = Accumulator(3)\n",
    "\n",
    "        net.train()\n",
    "        \n",
    "        for batch_idx, (x, y) in enumerate(data_loader):\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            data = x.to(device, non_blocking=True).to(torch.float)\n",
    "            label = y.to(device, non_blocking=True).to(torch.long)\n",
    "\n",
    "            pred = net(data)\n",
    "            \n",
    "            loss = loss_func(pred, label)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                metric.add(loss.sum(), accuracy(pred, label), x.shape[0])\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            train_l = metric[0] / metric[2]\n",
    "            train_acc = metric[1] / metric[2]\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        test_acc, test_l = evaluate_accuracy_gpu(net, test_iter, loss_func, device)\n",
    "\n",
    "        train_loss_history.append([epoch+1, train_l, test_l, train_acc, test_acc])\n",
    "\n",
    "        if (test_l <= min(np.asarray(train_loss_history)[:,1])):\n",
    "            save_model(epoch, net, optimizer, scheduler, \n",
    "                       checkpoint_dir=checkpoint_dir,\n",
    "                       train_loss_history=train_loss_history,\n",
    "                       filename=f'model_e{epoch}.pt',)\n",
    "            \n",
    "        print('Epoch: '+ epoch)\n",
    "        print(f'loss {train_l:.4f}, train acc {train_acc:.3f}, '\n",
    "              f'test acc {test_acc:.3f}')\n",
    "\n",
    "    return train_lost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0875f6d6-a7a4-47b5-87d3-7c0be4803ed1",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfd7dd2-5918-4b0c-806a-4dfe18d2630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "from ipynb.fs.defs.utils import *\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from ipynb.fs.full.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd70082-81ff-4400-960b-453f69341c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "'''\n",
    "\n",
    "def concat_csv_in_folder(folder_path):\n",
    "    csv_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "    df_concatenated = pd.concat((pd.read_csv(f) for f in csv_files), ignore_index=True)\n",
    "    return df_concatenated\n",
    "\n",
    "## directory\n",
    "data = concat_csv_in_folder(\"/Users/jianggh/Desktop/Gravity Spy Dataset/Data\")\n",
    "\n",
    "data = data[['ml_label','ml_confidence','url1','url2','url3','url4']]\n",
    "\n",
    "\n",
    "#Change ml_label column into one hot expression.\n",
    "unique_values = data['ml_label'].unique()\n",
    "\n",
    "'''\n",
    "one_hot_vectors = {val: np.zeros(len(unique_values)) for val in unique_values}\n",
    "for i, val in enumerate(unique_values):\n",
    "    one_hot_vectors[val][i] = 1\n",
    "\n",
    "data['ml_label'] = data['ml_label'].apply(lambda x: one_hot_vectors[x])\n",
    "\n",
    "#one hot into tensor. The loss function MSELoss in PyTorch accepts two tensor type.\n",
    "\n",
    "\n",
    "data['ml_label'] = data['ml_label'].apply(lambda x: torch.tensor(x, dtype=torch.float32))\n",
    "'''\n",
    "data.dropna()\n",
    "print(data.info())\n",
    "\n",
    "data['ml_confidence'] = data['ml_confidence'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b97e078-d7ad-4f77-b295-b2134bf91391",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a24f5b6-1f0a-443e-85be-5f119d341181",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device=torch.device('cuda')\n",
    "else:\n",
    "  device=torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0a834b-da38-4327-b0a2-d5abdd5f0c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = '/Users/jianggh/Desktop/Gravity Spy Dataset/Data'\n",
    "\n",
    "net = Baseline3()\n",
    "\n",
    "lr = 0.003\n",
    "total_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399337de-c8d2-4dfb-84f6-6b1652f9471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GS_Simple_Dataset3(train_data)\n",
    "test_dataset = GS_Simple_Dataset3(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78f7491-30fc-4801-8b94-bb333008330c",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindl = DataLoader(train_dataset, batch_size = 16, shuffle = True)\n",
    "testdl = DataLoader(test_dataset, batch_size = 16, shuffle = True)\n",
    "\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "history = train(net, lr, epoch = 0, total_epochs = total_epochs, dataset_train = train_dataset, data_loader = traindl, test_iter = testdl, train_loss_history = [],\n",
    "                checkpoint_dir = checkpoint_dir, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9727695-5796-40c8-addc-bfc3f3735680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory footprint support libraries/code\n",
    "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "!pip install gputil\n",
    "!pip install psutil\n",
    "!pip install humanize\n",
    "\n",
    "import psutil\n",
    "import humanize\n",
    "import os\n",
    "import GPUtil as GPU\n",
    "\n",
    "GPUs = GPU.getGPUs()\n",
    "# XXX: only one GPU on Colab and isn’t guaranteed\n",
    "gpu = GPUs[0]\n",
    "def printm():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    print(\"Gen RAM Free: \" + humanize.naturalsize(psutil.virtual_memory().available), \" |     Proc size: \" + humanize.naturalsize(process.memory_info().rss))\n",
    "    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total     {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
    "printm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10a14a0-5bb1-49d4-afe5-e7c5d282510a",
   "metadata": {},
   "source": [
    "### Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83318f76-e13d-4f8e-920d-20ad1b682f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainacc, trainloss, train_preds, train_labels = evaluate_accuracy_gpu(net, traindl, nn.CrossEntropyLoss(), device=device)\n",
    "\n",
    "all_preds = test_preds + train_preds\n",
    "all_labels = test_labels + train_labels\n",
    "\n",
    "n=0\n",
    "for i in range (len(all_preds)):\n",
    "    if all_preds[i]==all_labels[i]:\n",
    "        n+=1\n",
    "print(n,n/len(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b7faa3-81fa-48e0-8f53-79dd16e5fe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 24\n",
    "confusion_matrix = ConfusionMatrix(task='multiclass', num_classes=num_classes)\n",
    "confusion_matrix = confusion_matrix(torch.tensor(all_preds), torch.tensor(all_labels))\n",
    "\n",
    "confusion_matrix_row_normalized = confusion_matrix.float() \n",
    "row_sums = confusion_matrix_row_normalized.sum(dim=1, keepdim=True)\n",
    "confusion_matrix_row_normalized = confusion_matrix_row_normalized / row_sums\n",
    "confusion_matrix_row_normalized = confusion_matrix_row_normalized.numpy()\n",
    "\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "precision = precision_score(all_labels, all_preds, average=None)\n",
    "recall = recall_score(all_labels, all_preds, average=None)\n",
    "f1 = f1_score(all_labels, all_preds, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2cac9f-7f69-4dcf-a306-04dad196d330",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data['ml_label'].unique()\n",
    "ylabels = list(labels)\n",
    "ylabels.extend(['Recall', 'Precision'])\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "recall_matrix = recall.reshape(1,-1)\n",
    "precision_matrix = precision.reshape(1,-1)\n",
    "extended_matrix = np.vstack((confusion_matrix_row_normalized, recall_matrix, precision_matrix))\n",
    "\n",
    "\n",
    "ax = sns.heatmap(extended_matrix, annot=True, cmap='viridis', fmt=\".2f\",\n",
    "                 xticklabels=labels, yticklabels=ylabels,\n",
    "                 cbar_kws={'label': 'Value'},annot_kws={'size': 10, 'color': 'black'})\n",
    "\n",
    "ax.set_xlabel('Predicted Class')\n",
    "ax.set_ylabel('True Class')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08bde6a-5f50-4ebd-8cb3-86c9329d6ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/share/GS Testing Data/0.samples.csv\")\n",
    "\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "grouped = data.groupby('ml_label')\n",
    "\n",
    "for label, group in grouped:\n",
    "    part1 = group[group['ml_confidence'] <= 0.90].sample(n=100, random_state=42,replace=True)\n",
    "    part2 = group[group['ml_confidence'] > 0.90].sample(n=0, random_state=42)    \n",
    "    result_df = pd.concat([result_df, part1, part2])\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = torch.load('/share/GS Testing Data/model_6.19_3epochs0.01.pt')\n",
    "umapdataset = GS_Simple_Dataset(result_df,dic)\n",
    "dataloader = DataLoader(umapdataset, batch_size=32, shuffle=False)\n",
    "\n",
    "last_conv_outputs = []\n",
    "labels = []\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    last_conv_outputs.append(output.detach().cpu().numpy())\n",
    "\n",
    "last_conv_layer = model.features[-3]\n",
    "\n",
    "hook = last_conv_layer.register_forward_hook(hook_fn)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, label in tqdm(dataloader):\n",
    "        data = data.to(device).to(torch.float)\n",
    "        label = label.to(device).to(torch.long)\n",
    "        _ = model(data)\n",
    "        labels.extend(label.cpu().numpy())\n",
    "\n",
    "hook.remove()\n",
    "\n",
    "last_conv_outputs = np.concatenate([arr.reshape(arr.shape[0], -1) for arr in last_conv_outputs], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f610781-308b-4f4c-b2f7-06847525989d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=49.0, n_iter=1000)\n",
    "data_tsne = tsne.fit_transform(last_conv_outputs)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "unique_labels = np.unique(labels)\n",
    "\n",
    "colors = plt.cm.tab20b(np.linspace(0, 1, len(unique_labels)))\n",
    "for i, label in enumerate(unique_labels):\n",
    "    indices = np.where(labels == label)[0]\n",
    "    plt.scatter(data_tsne[indices, 0], data_tsne[indices, 1], s=100, color=colors[i], label=str(label), alpha=0.9)\n",
    "\n",
    "    \n",
    "    for x, y in data_tsne[indices]:\n",
    "        plt.text(x, y, str(label), fontsize=6, ha='center', va='center')\n",
    "    \n",
    "    \n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title('TSNE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cae3ebd-e055-4277-8c62-e0dce6856236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "\n",
    "data = np.load('/Users/jianggh/Desktop/lastconvoutputs.npy')\n",
    "labels = np.load('/Users/jianggh/Desktop/labels.npy')\n",
    "\n",
    "reducer = UMAP(n_components=2, n_neighbors=40, min_dist=0.1)\n",
    "data_umap = reducer.fit_transform(data)\n",
    "data_umap.shape\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "unique_labels = np.unique(labels)\n",
    "label_indices = {label: np.where(labels == label)[0] for label in unique_labels}\n",
    "\n",
    "# 绘制每个类别的点\n",
    "for label, indices in label_indices.items():\n",
    "    plt.scatter(data_umap[indices, 0], data_umap[indices, 1], label=str(label))\n",
    "\n",
    "# 添加图例\n",
    "plt.legend()\n",
    "\n",
    "# 设置标题和坐标轴\n",
    "plt.title('UMAP Visualization')\n",
    "plt.xlabel('UMAP Dimension 1')\n",
    "plt.ylabel('UMAP Dimension 2')\n",
    "\n",
    "# 显示图形\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28c7437-8e24-4a35-96c8-c9d3d4ea56c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
